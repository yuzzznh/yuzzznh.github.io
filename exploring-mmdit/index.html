<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing.">
  <meta name="keywords" content="Multimodal Diffusion, MM-DiT, Image Editing, Prompt-based Editing">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://joonghyuk.com/">Joonghyuk Shin</a><sup>1</sup>,</span>
            <span class="author-block">
              Alchan Hwang<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yuzzznh.github.io">Yujin Kim</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://carpedkm.github.io/">Daneul Kim</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://jaesik.info/">Jaesik Park</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Seoul National University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://yuzzznh.github.io"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper (Coming Soon)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://yuzzznh.github.io"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming Soon)</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://yuzzznh.github.io"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video (Coming Soon)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://yuzzznh.github.io"
                   class="external-link button is-normal is-rounded is-dark"
                   target="_blank">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.png" alt="Teaser image showing prompt-based editing examples" style="width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        Our prompt-based editing examples on SD3 series and Flux.1, demonstrating high-quality results across various editing scenarios.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <img src="./static/images/video-placeholder.jpg" alt="Steve video placeholder" style="width: 100%; height: 100%; object-fit: contain;">
        </div>
        <div class="item item-chair-tp">
          <img src="./static/images/video-placeholder.jpg" alt="Chair video placeholder" style="width: 100%; height: 100%; object-fit: contain;">
        </div>
        <div class="item item-shiba">
          <img src="./static/images/video-placeholder.jpg" alt="Shiba video placeholder" style="width: 100%; height: 100%; object-fit: contain;">
        </div>
        <div class="item item-fullbody">
          <img src="./static/images/video-placeholder.jpg" alt="Fullbody video placeholder" style="width: 100%; height: 100%; object-fit: contain;">
        </div>
        <div class="item item-blueshirt">
          <img src="./static/images/video-placeholder.jpg" alt="Blueshirt video placeholder" style="width: 100%; height: 100%; object-fit: contain;">
        </div>
        <div class="item item-mask">
          <img src="./static/images/video-placeholder.jpg" alt="Mask video placeholder" style="width: 100%; height: 100%; object-fit: contain;">
        </div>
        <div class="item item-coffee">
          <img src="./static/images/video-placeholder.jpg" alt="Coffee video placeholder" style="width: 100%; height: 100%; object-fit: contain;">
        </div>
        <div class="item item-toby">
          <img src="./static/images/video-placeholder.jpg" alt="Toby video placeholder" style="width: 100%; height: 100%; object-fit: contain;">
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Transformer-based diffusion models have recently superseded traditional U-Net
            architectures, with multimodal diffusion transformers (MM-DiT) emerging as
            the dominant approach in state-of-the-art models like Stable Diffusion 3 and
            Flux.1. Previous approaches have relied on unidirectional cross-attention
            mechanisms, with information flowing from text embeddings to image latents.
            In contrast, MM-DiT introduces a unified attention mechanism that concatenates
            input projections from both modalities and performs a single full attention operation,
            allowing bidirectional information flow between text and image branches. This
            architectural shift presents significant challenges for existing editing techniques.
            In this paper, we systematically analyze MM-DiT's attention mechanism by decomposing
            attention matrices into four distinct blocks, revealing their inherent
            characteristics. Through these analyses, we propose a robust, prompt-based
            image editing method for MM-DiT that supports global to local edits across
            various MM-DiT variants, including few-step models. We believe our findings
            bridge the gap between existing U-Net-based methods and emerging
            architectures, offering deeper insights into MM-DiT's behavioral patterns.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <p>Youtube video coming soon!</p>
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <h2 class="title is-3">Col2-1</h2>
          <p>
            Dummy text for col 2-1.
          </p>
          <img src="./static/images/video-placeholder.jpg" alt="Dolly zoom video placeholder" style="width: 100%; height: auto;">
        </div>
      </div>

      <div class="column">
        <h2 class="title is-3">Col2-2</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Dummy text for col 2-2.
            </p>
            <img src="./static/images/video-placeholder.jpg" alt="Matting video placeholder" style="width: 100%; height: auto;">
          </div>

        </div>
      </div>
    </div> -->

    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>

        <div class="content has-text-justified">
          <p>
            Our method leverages the unique architecture of multimodal diffusion transformers to enable prompt-based image editing.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/method-architecture-diagram.png" alt="Method architecture diagram" style="width: 100%; height: auto;">
          <p class="subtitle has-text-centered" style="margin-top: 1rem;">
            Architectural illustration of our method. We replace target (editing)
            branch projections q<sub>i</sub><sup>tgt</sup> and k<sub>i</sub><sup>tgt</sup>
            with their source branch counterparts, q<sub>i</sub><sup>src</sup> and
            k<sub>i</sub><sup>src</sup>. For local blending, we store the T2I
            portion of unmodified attention maps from selected blocks in both branches
            during early timesteps. A binary mask is computed from the union of these attention
            maps after thresholding and used to blend the two latent images. Best viewed
            zoomed in.
          </p>
        </div>

      </div>
    </div>

  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{shin2025exploring,
  author    = {Shin, Joonghyuk and Hwang, Alchan and Kim, Yujin and Kim, Daneul and Park, Jaesik},
  title     = {Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year      = {2025}
}</code></pre>
  </div>
</section>

</body>
</html>